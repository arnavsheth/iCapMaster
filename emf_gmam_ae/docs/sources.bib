@article{Albert1993,
   abstract = {We examine autoregressive time series models that are subject to regime switching. These shifts are determined by the outcome of an unobserved two-state indicator variable that follows a Markov process with unknown transition probabilities. A Bayesian framework is developed in which the unobserved states, one for each time point, are treated as missing data and then analyzed via the simulation tool of Gibbs sampling. This method is expedient because the conditional posterior distribution of the parameters, given the states, and the conditional posterior distribution of the states, given the parameters, all have a form amenable to Monte Carlo sampling. The approach is straightforward and generates marginal posterior distributions for all parameters of interest. Posterior distributions of the states, future observations, and the residuals, averaged over the parameter space are also obtained. Several examples with real and artificial data sets and weak prior information illustrate the usefulness of the methodology. An important problem in time series analysis, actually in virtually all of statistics, is the detection and modeling of abrupt changes in the model specified. Typically , interest centers on the behavior of the first few moments of the series-for example, the mean and the variance. The objective is to determine whether those moments are homogenous over time. Lack of homo-geneity, if not captured, can severely affect conclusions drawn from the data. Recognition of this fact has led to an interest in such nonlinear models as the bilinear (Tong 1983) and other models that allow for regime shifts or parameter instability (Tsurumi 1988). In this article, we focus on the autoregressive (AR) model with switching introduced by Sclove (1983) and Hamilton (1989). Both the mean and the variance of the real-valued time series are parameterized in terms of an unobserved state variable that follows a two-state Mar-kov process with unknown transition probabilities. In the engineering literature, related models, including the standard state-space model, are called hidden Markov models (Juang and Rabiner 1985) following the early canonical work of Baum and Eagon (1967). The Mar-kov switching AR model has been recently applied with success to several economic and financial data sets. For example, Hamilton (1988, 1989) used the model to date the timing of recessions and booms with gross national product (GNP) data and to model the term structure of interest rates, Pagan and Schwert (1990) used it to model stock returns, and other references may be found in the work of Hamilton (1991). Our objective here is to address, via Bayesian methods , inference issues that arise in the analysis of such models. The cornerstone of our approach is the idea that the unobserved states, one for each time point, can be treated as missing data and then analyzed, along with the other unknown parameters, via the simulation tool of Gibbs sampling There are a number of attractive features of this approach. First, the messy calculations entailed in the direct calculation of the likelihood function are avoided. Second, posterior distributions of all unknown parameters and functions thereof are obtained by simulating standard distributions, such as the multivariate normal and inverted gamma-these posterior distributions convey much more information than the mode and curvature summaries that arise from the maximum likelihood (ML) framework. Third, the approach provides posterior distributions of the states, and of future observations, marginalized over all of the unknown parameters. This improves on "plug-in" approaches in which unknown parameters-for example, those ap-1},
   author = {James H. Albert and Siddhartha Chib},
   doi = {10.2307/1391303},
   issn = {07350015},
   issue = {1},
   journal = {Journal of Business & Economic Statistics},
   keywords = {Data augmentation,Hidden Markov models,Missing data,Mixture distribution,Monte Carlo simulation,Regime shifts},
   month = {1},
   pages = {1},
   publisher = {Polson, and Stoffer},
   title = {Bayes Inference via Gibbs Sampling of Autoregressive Time Series Subject to Markov Mean and Variance Shifts},
   volume = {11},
   url = {https://www.jstor.org/stable/1391303?origin=crossref},
   year = {1993},
}
@book_section{Bai2021,
   abstract = {High-dimensional data sets have become ubiquitous in the past few decades, often with many more covariates than observations. In the frequentist setting, penalized likelihood methods are the most popular approach for variable selection and estimation in high-dimensional data. In the Bayesian framework, spike-and-slab methods are commonly used as probabilistic constructs for high-dimensional modeling. Within the context of linear regression, Rockova and George (2018) introduced the spike-and-slab LASSO (SSL), an approach based on a prior which provides a continuum between the penalized likelihood LASSO and the Bayesian point-mass spike-and-slab formulations. Since its inception, the spike-and-slab LASSO has been extended to a variety of contexts, including generalized linear models, factor analysis, graphical models, and nonparametric regression. The goal of this paper is to survey the landscape surrounding spike-and-slab LASSO methodology. First we elucidate the attractive properties and the computational tractability of SSL priors in high dimensions. We then review methodological developments of the SSL and outline several theoretical developments. We illustrate the methodology on both simulated and real datasets.},
   author = {Ray Bai and Veronika Ročková and Edward I. George},
   city = {Boca Raton},
   doi = {10.1201/9781003089018-4},
   journal = {Handbook of Bayesian Variable Selection},
   month = {12},
   pages = {81-108},
   publisher = {Chapman and Hall/CRC},
   title = {Spike-and-Slab Meets LASSO: A Review of the Spike-and-Slab LASSO},
   url = {https://www.taylorfrancis.com/books/9781003089018/chapters/10.1201/9781003089018-4},
   year = {2021},
}
@article{Bai2020,
   abstract = {We propose a variational Bayesian (VB) procedure for high-dimensional linear model inferences with heavy tail shrinkage priors, such as student-t prior. Theoretically, we establish the consistency of the proposed VB method and prove that under the proper choice of prior specifications, the contraction rate of the VB posterior is nearly optimal. It justifies the validity of VB inference as an alternative of Markov Chain Monte Carlo (MCMC) sampling. Meanwhile, comparing to conventional MCMC methods, the VB procedure achieves much higher computational efficiency, which greatly alleviates the computing burden for modern machine learning applications such as massive data analysis. Through numerical studies, we demonstrate that the proposed VB method leads to shorter computing time, higher estimation accuracy, and lower variable selection error than competitive sparse Bayesian methods.},
   author = {Jincheng Bai and Qifan Song and Guang Cheng},
   month = {10},
   title = {Nearly Optimal Variational Inference for High Dimensional Regression with Shrinkage Priors},
   url = {http://arxiv.org/abs/2010.12887},
   year = {2020},
}
@report{Barnard2000,
   abstract = {The covariance matrix plays an important role in statistical inference, yet modeling a covariance matrix is often a difficult task in practice due to its dimensionality and the non-negative definite constraint. In order to model a co-variance matrix effectively, it is typically broken down into components based on modeling considerations or mathematical convenience. Decompositions that have received recent research attention include variance components, spectral decomposition , Cholesky decomposition, and matrix logarithm. In this paper we study a statistically motivated decomposition which appears to be relatively unexplored for the purpose of modeling. We model a covariance matrix in terms of its corresponding standard deviations and correlation matrix. We discuss two general modeling situations where this approach is useful: shrinkage estimation of regression coefficients , and a general location-scale model for both categorical and continuous variables. We present some simple choices for priors in terms of standard deviations and the correlation matrix, and describe a straightforward computational strategy for obtaining the posterior of the covariance matrix. We apply our method to real and simulated data sets in the context of shrinkage estimation. 1. A Separation Strategy for Modeling Covariance Matrices Modeling a variance-covariance structure is one of the most common and important tasks in statistical analysis. It is also one of the most difficult. A covariance matrix may have many parameters, and these parameters are constrained by the complex requirement that the matrix be non-negative definite. In this paper we investigate a simple strategy that attempts to deal with these problems in some applications. Although our focus is on Bayesian analysis, our strategy is equally applicable for non-Bayesian modeling; in Section 4, we give one such application, which involves extensions to the general location model. Our strategy includes a simple method for computing the posterior of a covariance matrix using the Gibbs sampler.},
   author = {John Barnard and Robert Mcculloch and Xiao-Li Meng},
   journal = {Statistica Sinica},
   keywords = {Gibbs sampler,Markov chain Monte Carlo,Wishart distri-bution,and phrases: General location model,general location-scale model,hierarchical models},
   pages = {1281-1311},
   title = {Modeling Covariance Matrices in Terms of Standard Deviations and Correlations, with Application to Shrinkage},
   volume = {10},
   year = {2000},
}
@article{Belitser2015,
   abstract = {In the general signal+noise model we construct an empirical Bayes posterior which we then use for uncertainty quantification for the unknown, possibly sparse, signal. We introduce a novel excessive bias restriction (EBR) condition, which gives rise to a new slicing of the entire space that is suitable for uncertainty quantification. Under EBR and some mild conditions on the noise, we establish the local (oracle) optimality of the proposed confidence ball. In passing, we also get the local optimal (oracle) results for estimation and posterior contraction problems. Adaptive minimax results (also for the estimation and posterior contraction problems) over various sparsity classes follow from our local results.},
   author = {Eduard Belitser and Nurzhan Nurushev},
   month = {11},
   title = {Needles and straw in a haystack: robust confidence for possibly sparse sequences},
   url = {http://arxiv.org/abs/1511.01803},
   year = {2015},
}
@book{Bishop2006,
   author = {Christopher Bishop},
   title = {Pattern Recognition and Machine Learning},
   year = {2006},
}
@article{Blei2017,
   abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
   author = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
   doi = {10.1080/01621459.2017.1285773},
   issn = {0162-1459},
   issue = {518},
   journal = {Journal of the American Statistical Association},
   month = {4},
   pages = {859-877},
   title = {Variational Inference: A Review for Statisticians},
   volume = {112},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
   year = {2017},
}
@article{Boscardin1994,
   abstract = {In the linear model with unknown variances, one can often model the heteroscedasticity as var(y i) = 2 f(w i ;); where f is a xed function, w i are the \weights" for the problem and is an unknown parameter (f(w i ;) = w i is a traditional choice). We show how to do a fully Bayesian computation in this simple linear setting and also for a hierarchical model. The full Bayesian computation has the advantage that we are able to average over our uncertainty in instead of using a point estimate. We carry out the computations for a problem involving forecasting U.S. Presidential elections, looking at diierent choices for f and the eeects on both estimation and prediction.},
   author = {W John Boscardin and Andrew Gelman},
   journal = {Advances in Econometrics},
   title = {Bayesian Computation for Parametric Models of Heteroscedasticity in the Linear Model},
   year = {1994},
}
@article{Carbonetto2012,
   abstract = {The Bayesian approach to variable selection in regression is a powerful tool for tackling many scientific problems. Inference for variable selection models is usually implemented using Markov chain Monte Carlo (MCMC). Because MCMC can impose a high computational cost in studies with a large number of variables, we assess an alternative to MCMC based on a simple variational approximation. Our aim is to retain useful features of Bayesian variable selection at a reduced cost. Using simulations designed to mimic genetic association studies, we show that this simple variational approximation yields posterior inferences in some settings that closely match exact values. In less restrictive (and more realistic) conditions, we show that posterior probabilities of inclusion for individual variables are often incorrect, but variational estimates of other useful quantities|including posterior distributions of the hyperparameters|are remarkably accurate. We illustrate how these results guide the use of variational inference for a genome-wide association study with thousands of samples and hundreds of thousands of variables. © 2012 International Society for Bayesian Analysis.},
   author = {Peter Carbonetto and Matthew Stephens},
   doi = {10.1214/12-BA703},
   issn = {19360975},
   issue = {1},
   journal = {Bayesian Analysis},
   keywords = {Genetic association studies,Monte carlo,Variable selection,Variational inference},
   pages = {73-108},
   title = {Scalable variational inference for bayesian variable selection in regression, and its accuracy in genetic association studies},
   volume = {7},
   year = {2012},
}
@report{Carlint1995,
   abstract = {Markov chain Monte Carlo (MCMC) integration methods enable the fitting of models of virtually unlimited complexity, and as such have revolutionized the practice of Bayesian data analysis. However, comparison across models may not proceed in a completely analogous fashion, owing to violations of the conditions sufficient to ensure convergence of the Markov chain. In this paper we present a framework for Bayesian model choice, along with an MCMC algorithm that does not suffer from convergence difficulties. Our algorithm applies equally well to problems where only one model is contemplated but its proper size is not known at the outset, such as problems involving integer-valued parameters, multiple changepoints or finite mixture distributions. We illustrate our approach with two published examples.},
   author = {Bradley P Carlint},
   issue = {3},
   journal = {J. R. Statist. Soc. B},
   keywords = {BAYES FACTOR,FINITE MIXTURE MODEL,GIBBS SAMPLER,INTEGER-VALUED PARAMETERS,MODELS OF VARYING SIZE,MULTIPLE CHANGEPOINT MODEL,NON-NESTED MODELS},
   pages = {473-484},
   title = {Bayesian Model Choice via Markov Chain Monte Carlo Methods},
   volume = {57},
   year = {1995},
}
@article{Chib1993,
   abstract = {This paper develops a practical framework for the Bayesian analysis of Gaussian and Student-c regression models with autocorrelated errors. As is customary in classical estimation procedures, the posteriors are conditioned on the initial observations. Recourse is taken to the method of Gibbs sampling, an iterative Markovian sampling method, and it is shown that the proposed approach can readily deal with high-order autoregressive processes without requiring an importance sampling function or other tuning constants. Several examples, including one with AR(4) errors, are used to illustrate the ideas.},
   author = {Siddhartha Chib},
   journal = {Journal of Econometrics},
   pages = {275-294},
   title = {Bayes regression with autoregressive errors: A Gibbs sampling approach},
   volume = {58},
   year = {1993},
}
@article{Chib1994,
   abstract = {We develop practical and exact methods of analyzing ARMA(p, 4) regression error models in a Bayesian framework by using the Gibbs sampling and Metropolis-Hastings algorithms, and we prove that the kernel of the proposed Markov chain sampler converges to the true density. The procedures can be applied to pure ARMA time series models and to determine features of the likelihood function by choosing appropriate diffuse priors. Our results are unconditional on the initial observations. We also show how the algorithm can be further simplified for the important special cases of stationary AR(p) and invertible MA(q) models. Recursive transformations developed in this paper to diagonalize the covariance matrix of the errors should prove useful in frequentist estimation. Examples with simulated and actual economic data are presented.},
   author = {Siddhartha Chib and Edward Greenberg},
   doi = {10.1016/0304-4076(94)90063-9},
   issn = {03044076},
   issue = {1-2},
   journal = {Journal of Econometrics},
   keywords = {ARMA processes,Bayesian statistics JEL classification: Cl 1,C15,C22,Data augmentation,KPJ~ ~~ourls: Gibbs sampling,Markov chain,Metropolis-Hastings algorithm,Time series},
   month = {9},
   pages = {183-206},
   title = {Bayes inference in regression models with ARMA (p, q) errors},
   volume = {64},
   url = {https://linkinghub.elsevier.com/retrieve/pii/0304407694900639},
   year = {1994},
}
@working_paper{Fazelnia2018,
   abstract = {We present a new technique for solving non-convex variational inference optimization problems. Variational inference is a widely used method for posterior approximation in which the inference problem is transformed into an optimization problem. For most models, this optimization is highly non-convex and so hard to solve. In this paper, we introduce a new approach to solving the variational inference optimization based on convex relaxation and semidefinite programming. Our theoretical results guarantee very tight relaxation bounds that get nearer to the global optimal solution than traditional coordinate ascent. We evaluate the performance of our approach on regression and sparse coding.},
   author = {Ghazal Fazelnia and John Paisley},
   title = {CRVI: Convex Relaxation for Variational Inference},
   year = {2018},
}
@report{Fink1997,
   abstract = {This report reviews conjugate priors and priors closed under sampling for a variety of data generating processes where the prior distributions are univariate, bivariate, and multivariate. The effects of transformations on conjugate prior relationships are considered and cases where conjugate prior relationships can be applied under transformations are identified. Univariate and bivariate prior relationships are verified using Monte Carlo methods.},
   author = {Daniel Fink},
   title = {A Compendium of Conjugate Priors},
   year = {1997},
}
@article{Gelman2013,
   abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a biascorrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this review is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
   author = {Andrew Gelman and Jessica Hwang and Aki Vehtari},
   doi = {https://doi.org/10.48550/arXiv.1307.5928},
   keywords = {AIC,Bayes,DIC,WAIC,cross-validation,prediction},
   month = {7},
   title = {Understanding predictive information criteria for Bayesian models},
   url = {http://arxiv.org/abs/1307.5928},
   year = {2013},
}
@report{George1997,
   abstract = {This paper describes and compares various hierarchical mixture prior formulations of variable selection uncertainty in normal linear regression models. These include the nonconjugate SSVS formulation of George and McCulloch (1993), as well as conjugate formulations which allow for analytical simplification. Hyperpa-rameter settings which base selection on practical significance, and the implications of using mixtures with point priors are discussed. Computational methods for posterior evaluation and exploration are considered. Rapid updating methods are seen to provide feasible methods for exhaustive evaluation using Gray Code sequencing in moderately sized problems, and fast Markov Chain Monte Carlo exploration in large problems. Estimation of normalization constants is seen to provide improved posterior estimates of individual model probabilities and the total visited probability. Various procedures are illustrated on simulated sample problems and on a real problem concerning the construction of financial index tracking portfolios.},
   author = {Edward I George and Robert E Mcculloch},
   journal = {Statistica Sinica},
   keywords = {Gibbs sampling,Gray Code,Markov chain Monte Carlo,Metropolis-Hastings algorithms,and phrases: Conjugate prior,hierarchi-cal models,normal mixtures,normalization constant,regression,simulation},
   pages = {339-373},
   title = {Approaches for Bayesian Variable Selection},
   volume = {7},
   year = {1997},
}
@article{George1993,
   author = {Edward I. George and Robert E. McCulloch},
   doi = {10.1080/01621459.1993.10476353},
   issn = {0162-1459},
   issue = {423},
   journal = {Journal of the American Statistical Association},
   month = {9},
   pages = {881-889},
   title = {Variable Selection via Gibbs Sampling},
   volume = {88},
   url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476353},
   year = {1993},
}
@article{Geweke1993,
   author = {John Geweke},
   journal = {Journal of Applied Econometrics},
   title = {Bayesian Treatment of the Independent Student-t Linear Model},
   year = {1993},
}
@report{Hastie2017,
   author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
   title = {Springer Series in Statistics The Elements of Statistical Learning Data Mining, Inference, and Prediction},
   year = {2017},
}
@report{Hisakado2006,
   abstract = {We discuss a general method to construct correlated binomial distributions by imposing several consistent relations on the joint probability function. We obtain self-consistency relations for the conditional correlations and conditional probabilities. The beta-binomial distribution is derived by a strong symmetric assumption on the conditional correlations. Our derivation clarifies the 'correlation' structure of the beta-binomial distribution. It is also possible to study the correlation structures of other probability distributions of exchangeable (homogeneous) correlated Bernoulli random variables. We study some distribution functions and discuss their behaviors in terms of their correlation structures.},
   author = {Masato Hisakado and Kenji Kitsukawa and Shintaro Mori},
   keywords = {0250Cw,PACS numbers},
   title = {Correlated Binomial Models and Correlation Structures},
   year = {2006},
}
@article{Huang2016,
   abstract = {There has been an intense development on the estimation of a sparse regression coefficient vector in statistics, machine learning and related fields. In this paper, we focus on the Bayesian approach to this problem, where sparsity is incorporated by the so-called spike-and-slab prior on the coefficients. Instead of replying on MCMC for posterior inference, we propose a fast and scalable algorithm based on variational approximation to the posterior distribution. The updating scheme employed by our algorithm is different from the one proposed by Carbonetto and Stephens (2012). Those changes seem crucial for us to show that our algorithm can achieve asymptotic consistency even when the feature dimension diverges exponentially fast with the sample size. Empirical results have demonstrated the effectiveness and efficiency of the proposed algorithm.},
   author = {Xichen Huang and Jin Wang and Feng Liang},
   month = {2},
   title = {A Variational Algorithm for Bayesian Variable Selection},
   url = {http://arxiv.org/abs/1602.07640},
   year = {2016},
}
@report{Hussein2008,
   abstract = {The problem of estimating a set of parameters in the autoregressive moving average model with exogenous inputs (ARMAX) is considered and a numerical Bayesian method proposed. This paper, develops a Bayesian analysis for the ARMAX model by implementing a fast, easy and accurate Gibbs sampling algorithm. The procedure is easy to implement and can be computed also when some priors in the ARMAX are diffuse. The empirical results of the simulated examples and electricity consumption data in the industrial sector in Egypt showed the accuracy of the proposed methodology and has good statistical properties.},
   author = {Hassan M A Hussein},
   issue = {12},
   journal = {Journal of Applied Sciences Research},
   keywords = {Gibbs Sampling},
   pages = {1885-1892},
   title = {Bayesian Analysis of the Autoregressive-Moving Average Model with Exogenous Inputs Using Gibbs Sampling},
   volume = {4},
   year = {2008},
}
@article{Ishwaran2005,
   abstract = {DNA microarrays can provide insight into genetic changes that characterize different stages of a disease process. Accurate identification of these changes has significant therapeutic and diagnostic implications. Statistical analysis for multistage (multigroup) data is challenging, however. ANOVA-based extensions of two-sample Z-tests, a popular method for detecting differentially expressed genes in two groups, do not work well in multigroup settings. False detection rates are high because of variability of the ordinary least squares estimators and because of regression to the mean induced by correlated parameter estimates. We develop a Bayesian rescaled spike and slab hierarchical model specifically designed for the multigroup gene detection problem. Data preprocessing steps are introduced to deal with unique features of microarray data and to enhance selection performance. We show theoretically that spike and slab models naturally encourage sparse solutions through a process called selective shrinkage. This translates into oracle-like gene selection risk performance compared with ordinary least squares estimates. The methodology is illustrated on a large microarray repository of samples from different clinical stages of metastatic colon cancer. Through a functional analysis of selected genes, we show that spike and slab models identify important biological signals while minimizing biologically implausible false detections. © 2005 American Statistical Association.},
   author = {Hemant Ishwaran and J. Sunil Rao},
   doi = {10.1198/016214505000000051},
   issn = {0162-1459},
   issue = {471},
   journal = {Journal of the American Statistical Association},
   keywords = {Colon cancer,Hypervariance,Penalization,Rescaling,Risk misclassification,Shrinkage,Sparsity,Stochastic variable selection,Zcut},
   month = {9},
   pages = {764-780},
   title = {Spike and Slab Gene Selection for Multigroup Microarray Data},
   volume = {100},
   url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000000051},
   year = {2005},
}
@working_paper{Karlsson2012,
   abstract = {Prepared for the Handbook of Economic Forecasting, vol 2 This chapter reviews Bayesian methods for inference and forecasting with VAR models. Bayesian inference and, by extension, forecasting depends on numerical methods for simulating from the posterior distribution of the parameters and special attention is given to the implementation of the simulation algorithm. JEL-codes: C11, C32, C53},
   author = {Sune Karlsson},
   issn = {1403-0586},
   keywords = {Cointegration,Condi-tional forecasts,Large VAR * SuneKarlsson@oruse,Markov chain Monte Carlo,Model selection,Stochastic volatility,Structural VAR,Time-varying parameters},
   title = {Forecasting with Bayesian Vector Autoregressions},
   url = {http://www.oru.se/Institutioner/Handelshogskolan-vid-Orebro-universitet/Forskning/Publikationer/Working-papers/},
   year = {2012},
}
@book{Koop2003,
   author = {Gary Koop},
   isbn = {0-470-84567-8},
   title = {Bayesian Econometrics},
   year = {2003},
}
@article{Liang2008,
   abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures. © 2008 American Statistical Association.},
   author = {Feng Liang and Rui Paulo and German Molina and Merlise A. Clyde and Jim O. Berger},
   doi = {10.1198/016214507000001337},
   issn = {01621459},
   issue = {481},
   journal = {Journal of the American Statistical Association},
   keywords = {AIC,BIC,Bayesian model averaging,Cauchy,Empirical bayes,Gaussian hypergeometric functions,Model selection,Zellner-Siow priors},
   month = {3},
   pages = {410-423},
   title = {Mixtures of g priors for Bayesian variable selection},
   volume = {103},
   year = {2008},
}
@article{Logsdon2010,
   abstract = {Background: The success achieved by genome-wide association (GWA) studies in the identification of candidate loci for complex diseases has been accompanied by an inability to explain the bulk of heritability. Here, we describe the algorithm V-Bay, a variational Bayes algorithm for multiple locus GWA analysis, which is designed to identify weaker associations that may contribute to this missing heritability. Results: V-Bay provides a novel solution to the computational scaling constraints of most multiple locus methods and can complete a simultaneous analysis of a million genetic markers in a few hours, when using a desktop. Using a range of simulated genetic and GWA experimental scenarios, we demonstrate that V-Bay is highly accurate, and reliably identifies associations that are too weak to be discovered by single-marker testing approaches. V-Bay can also outperform a multiple locus analysis method based on the lasso, which has similar scaling properties for large numbers of genetic markers. For demonstration purposes, we also use V-Bay to confirm associations with gene expression in cell lines derived from the Phase II individuals of HapMap. Conclusions: V-Bay is a versatile, fast, and accurate multiple locus GWA analysis tool for the practitioner interested in identifying weaker associations without high false positive rates.},
   author = {Benjamin A Logsdon and Gabriel E Hoffman and Jason G Mezey},
   doi = {10.1186/1471-2105-11-58},
   issn = {1471-2105},
   issue = {1},
   journal = {BMC Bioinformatics},
   month = {12},
   pages = {58},
   title = {A variational Bayes algorithm for fast and accurate multiple locus genome-wide association analysis},
   volume = {11},
   url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-58},
   year = {2010},
}
@article{,
   abstract = {An important task in building regression models is to decide which regressors should be included in the final model. In a Bayesian approach, variable selection can be performed using mixture priors with a spike and a slab component for the effects subject to selection. As the spike is concentrated at zero, variable selection is based on the probability of assigning the corresponding regression effect to the slab component. These posterior inclusion probabilities can be determined by MCMC sampling. In this paper we compare the MCMC implementations for several spike and slab priors with regard to posterior inclusion probabilities and their sampling efficiency for simulated data. Further, we investigate posterior inclusion probabilities analytically for different slabs in two simple settings. Application of variable selection with spike and slab priors is illustrated on a data set of psychiatric patients where the goal is to identify covariates affecting metabolism. Zusammenfassung: Ein wesentliches Problem der Regressionsmodellierung ist die Auswahl der Regressoren, die ins Modell aufgenommen werden. In einem Bayes-Ansatz kann Variablenselektion durchgeführt werden, indem als a-priori-Verteilung für die Regressionseffekte der in Frage kommenden Variablen eine Mischverteilung mit zwei Komponenten gewählt wird: die erste Komponente mit einer Spitze bei Null wird als "spike", die zweite flache Komponente als "slab" bezeichnet. Die Selektion der Variablen erfolgt dann auf Basis der posteriori Wahrscheinlichkeit, mit der ein Effekt der slab-Komponente zugeordnet wird. Diese sogenannten Inklusionswahrscheinlich-keiten können mit Hilfe der MCMC-Ziehungen geschätzt werden. Im vor-liegenden Beitrag werden MCMC-Implementierungen für verschiedene spike-and-slab-Verteilungen hinsichtlich der Inklusionswahrscheinlichkeiten und der Effizienz ihrer Schätzung anhand von simulierten Daten verglichen. Außer-dem untersuchen wir die Inklusionswahrscheinlichkeiten für verschiedene Slab-Komponenten in zwei einfachen Fällen auch analytisch. Schließlich wird Variablenselektion mit Spike-and-Slab-Priori-Verteilungen auf einen me-dizinischen Datensatz angewendet, um Regressoren, die den Stoffwechsel von psychiatrischen Patienten beeinflussen, zu indetnifizieren.},
   author = {Gertraud Malsiner-Walli and Helga Wagner and Johannes Kepler},
   journal = {Austrian Journal of Statistics},
   keywords = {Dirac Spike,NMIG prior,Normal Scale Mixtures,Poste-rior Inclusion Probability,SSVS},
   pages = {241-264},
   title = {Comparing Spike and Slab Priors for Bayesian Variable Selection},
   volume = {40},
   year = {2011},
}
@report{Martin2020,
   abstract = {In this paper we adopt the familiar sparse, high-dimensional linear regression model and focus on the important but often overlooked task of prediction. In particular, we consider a new empirical Bayes framework that incorporates data in the prior in two ways: one is to center the prior for the non-zero regression coefficients and the other is to provide some additional regularization. We show that, in certain settings, the asymptotic concentration of the proposed empirical Bayes posterior predictive distribution is very fast, and we establish a Bernstein-von Mises theorem which ensures that the derived empirical Bayes prediction intervals achieve the targeted frequentist coverage probability. The empirical prior has a convenient conjugate form, so posterior computations are relatively simple and fast. Finally, our numerical results demonstrate the proposed method's strong finite-sample performance in terms of prediction accuracy, uncertainty quantification, and computation time compared to existing Bayesian methods.},
   author = {Ryan Martin and Yiqi Tang},
   journal = {Journal of Machine Learning Research},
   keywords = {Bayesian inference,data-dependent prior,model averaging,predictive distri-bution,uncertainty quantification},
   pages = {1-30},
   title = {Empirical Priors for Prediction in Sparse High-dimensional Linear Regression},
   volume = {21},
   url = {http://jmlr.org/papers/v21/19-152.html.},
   year = {2020},
}
@article{Neville2014,
   abstract = {We investigate mean field variational approximate Bayesian inference for models that use continuous distributions, Horseshoe, Negative-Exponential-Gamma and Generalized Double Pareto, for sparse signal shrinkage. Our principal finding is that the most natural, and simplest, mean field variational Bayes algorithm can perform quite poorly due to posterior dependence among auxiliary variables. More sophisticated algorithms, based on special functions, are shown to be superior. Continued fraction approximations via Lentz's Algorithm are developed to make the algorithms practical.},
   author = {Sarah E. Neville and John T. Ormerod and M. P. Wand},
   doi = {10.1214/14-EJS910},
   issn = {1935-7524},
   issue = {1},
   journal = {Electronic Journal of Statistics},
   keywords = {Approximate Bayesian inference,Generalized Double Par-eto distribution,Horseshoe distribution,Lentz's Algorithm,Normal-Exponential-Gamma distribution,continued fraction,special function},
   month = {1},
   title = {Mean field variational Bayes for continuous sparse signal shrinkage: Pitfalls and remedies},
   volume = {8},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-8/issue-1/Mean-field-variational-Bayes-for-continuous-sparse-signal-shrinkage/10.1214/14-EJS910.full},
   year = {2014},
}
@article{Ormerod2017,
   abstract = {We develop methodology and theory for a mean field variational Bayes approximation to a linear model with a spike and slab prior on the regression coefficients. In particular we show how our method forces a subset of regression coefficients to be numerically indistinguishable from zero; under mild regularity conditions estimators based on our method consistently estimate the model parameters with easily obtainable and (asymptotically) appropriately sized standard error estimates; and select the true model at an exponential rate in the sample size. We also develop a practical method for simultaneously choosing reasonable initial parameter values and tuning the main tuning parameter of our algorithms which is both computationally efficient and empirically performs as well or better than some popular variable selection approaches. Our method is also faster and highly accurate when compared to MCMC.},
   author = {John T. Ormerod and Chong You and Samuel Müller},
   doi = {10.1214/17-EJS1332},
   issn = {19357524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   keywords = {Bernoulli-Gaussian model,Markov chain monte carlo,Mean field variational bayes},
   pages = {3549-3594},
   publisher = {Institute of Mathematical Statistics},
   title = {A variational bayes approach to variable selection},
   volume = {11},
   year = {2017},
}
@article{Ormerod2010,
   abstract = {Variational approximations facilitate approximate inference for the parameters in complex statistical models and provide fast, deterministic alternatives to Monte Carlo methods. However, much of the contemporary literature on variational approximations is in Computer Science rather than Statistics, and uses terminology, notation, and examples from the former field. In this article we explain variational approximation in statistical terms. In particular, we illustrate the ideas of variational approximation using examples that are familiar to statisticians. © 2010 American Statistical Association.},
   author = {J. T. Ormerod and M. P. Wand},
   doi = {10.1198/tast.2010.09058},
   issn = {00031305},
   issue = {2},
   journal = {American Statistician},
   keywords = {Bayesian inference,Bayesian networks,Directed acyclic graphs,Generalized linear mixed models,Kullback,Leibler divergence,Linear mixed models},
   month = {5},
   pages = {140-153},
   title = {Explaining variational approximations},
   volume = {64},
   year = {2010},
}
@article{Piironen2017,
   abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
   author = {Juho Piironen and Aki Vehtari},
   doi = {10.1007/s11222-016-9649-y},
   issn = {0960-3174},
   issue = {3},
   journal = {Statistics and Computing},
   month = {5},
   pages = {711-735},
   title = {Comparison of Bayesian predictive methods for model selection},
   volume = {27},
   url = {http://link.springer.com/10.1007/s11222-016-9649-y},
   year = {2017},
}
@article{Piironen2017,
   abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
   author = {Juho Piironen and Aki Vehtari},
   doi = {10.1214/17-EJS1337SI},
   issn = {1935-7524},
   issue = {2},
   journal = {Electronic Journal of Statistics},
   month = {1},
   title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
   volume = {11},
   url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full},
   year = {2017},
}
@article{Plummer2020,
   abstract = {Variational algorithms have gained prominence over the past two decades as a scalable computational environment for Bayesian inference. In this article, we explore tools from the dynamical systems literature to study the convergence of coordinate ascent algorithms for mean field variational inference. Focusing on the Ising model defined on two nodes, we fully characterize the dynamics of the sequential coordinate ascent algorithm and its parallel version. We observe that in the regime where the objective function is convex, both the algorithms are stable and exhibit convergence to the unique fixed point. Our analyses reveal interesting discordances between these two versions of the algorithm in the region when the objective function is non-convex. In fact, the parallel version exhibits a periodic oscillatory behavior which is absent in the sequential version. Drawing intuition from the Markov chain Monte Carlo literature, we empirically show that a parameter expansion of the Ising model, popularly called the Edward–Sokal coupling, leads to an enlargement of the regime of convergence to the global optima.},
   author = {Sean Plummer and Debdeep Pati and Anirban Bhattacharya},
   doi = {10.3390/e22111263},
   issn = {10994300},
   issue = {11},
   journal = {Entropy},
   keywords = {Bifurcation,Dynamical systems,Edward–Sokal coupling,Kullback–Leibler divergence,Mean-field,Variational inference},
   month = {11},
   pages = {1-33},
   publisher = {MDPI AG},
   title = {Dynamics of coordinate ascent variational inference: A case study in 2D Ising models},
   volume = {22},
   year = {2020},
}
@article{Ray2021,
   abstract = {We study a mean-field spike and slab variational Bayes (VB) approximation to Bayesian model selection priors in sparse high-dimensional linear regression. Under compatibility conditions on the design matrix, oracle inequalities are derived for the mean-field VB approximation, implying that it converges to the sparse truth at the optimal rate and gives optimal prediction of the response vector. The empirical performance of our algorithm is studied, showing that it works comparably well as other state-of-the-art Bayesian variable selection methods. We also numerically demonstrate that the widely used coordinate-ascent variational inference algorithm can be highly sensitive to the parameter updating order, leading to potentially poor performance. To mitigate this, we propose a novel prioritized updating scheme that uses a data-driven updating order and performs better in simulations. The variational algorithm is implemented in the R package sparsevb. Supplementary materials for this article are available online.},
   author = {Kolyan Ray and Botond Szabó},
   doi = {10.1080/01621459.2020.1847121},
   issn = {0162-1459},
   journal = {Journal of the American Statistical Association},
   keywords = {Model selection,Oracle inequalities,Sparsity,Spike-and-slab prior,Variational Bayes},
   month = {1},
   pages = {1-12},
   publisher = {American Statistical Association},
   title = {Variational Bayes for High-Dimensional Linear Regression With Sparse Priors},
   url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1847121},
   year = {2021},
}
@article{,
   abstract = {Despite rapid developments in stochastic search algorithms, the practicality of Bayesian variable selection methods has continued to pose challenges. High-dimensional data are now routinely analyzed, typically with many more covariates than observations. To broaden the applicability of Bayesian variable selection for such high-dimensional linear regression contexts, we propose EMVS, a deterministic alternative to stochastic search based on an EM algorithm which exploits a conjugate mixture prior formulation to quickly find posterior modes. Combining a spike-and-slab regularization diagram for the discovery of active predictor sets with subsequent rigorous evaluation of posteriormodel probabilities, EMVSrapidly identifies promising sparse high posterior probability submodels. External structural information such as likely covariate groupings or network topologies is easily incorporated into the EMVS framework. Deterministic annealing variants are seen to improve the effectiveness of our algorithms by mitigating the posterior multimodality associated with variable selection priors. The usefulness of the EMVS approach is demonstrated on real high-dimensional data, where computational complexity renders stochastic search to be less practical.},
   author = {Veronika Ročková and Edward I. George},
   doi = {10.1080/01621459.2013.869223},
   issn = {0162-1459},
   issue = {506},
   journal = {Journal of the American Statistical Association},
   keywords = {Dynamic posterior exploration,High dimensionality,Regularization plots,SSVS,Sparsity},
   month = {4},
   pages = {828-846},
   publisher = {American Statistical Association},
   title = {EMVS: The EM Approach to Bayesian Variable Selection},
   volume = {109},
   url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2013.869223},
   year = {2014},
}
@article{Scott2010,
   abstract = {This paper studies the multiplicity-correction effect of standard Bayesian variable-selection priors in linear regression. Our first goal is to clarify when, and how, multiplicity correction happens automatically in Bayesian analysis, and to distinguish this correction from the Bayesian Ockham's-razor effect. Our second goal is to contrast empirical-Bayes and fully Bayesian approaches to variable selection through examples, theoretical results and simulations. Considerable differences between the two approaches are found. In particular, we prove a theorem that characterizes a surprising aymptotic discrepancy between fully Bayes and empirical Bayes. This discrepancy arises from a different source than the failure to account for hyperparameter uncertainty in the empirical-Bayes estimate. Indeed, even at the extreme, when the empirical-Bayes estimate converges asymptotically to the true variable-inclusion probability, the potential for a serious difference remains. © Institute of Mathematical Statistics, 2010.},
   author = {James G. Scott and James O. Berger},
   doi = {10.1214/10-AOS792},
   issn = {00905364},
   issue = {5},
   journal = {Annals of Statistics},
   keywords = {Bayesian model selection,Empirical Bayes,Multiple testing,Variable selection},
   month = {10},
   pages = {2587-2619},
   title = {Bayes and empirical-Bayes multiplicity adjustment in the variable-selection problem},
   volume = {38},
   year = {2010},
}
@report{Teh2008,
   author = {Yee Whye Teh and Michael I Jordan},
   title = {Hierarchical Bayesian Nonparametric Models with Applications},
   year = {2008},
}
@working_paper{Titsias2011,
   abstract = {We introduce a variational Bayesian inference algorithm which can be widely applied to sparse linear models. The algorithm is based on the spike and slab prior which, from a Bayesian perspective, is the golden standard for sparse inference. We apply the method to a general multi-task and multiple kernel learning model in which a common set of Gaussian process functions is linearly combined with task-specific sparse weights, thus inducing relation between tasks. This model unifies several sparse linear models, such as generalized linear models, sparse factor analysis and matrix factorization with missing values, so that the variational algorithm can be applied to all these cases. We demonstrate our approach in multi-output Gaussian process regression, multi-class classification, image processing applications and collaborative filtering.},
   author = {Michalis K Titsias and Miguel Lázaro-Gredilla},
   title = {Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning},
   year = {2011},
}
@article{Tomasetti2019,
   abstract = {Variational Bayesian (VB) methods produce posterior inference in a time frame considerably smaller than traditional Markov Chain Monte Carlo approaches. Although the VB posterior is an approximation, it has been shown to produce good parameter estimates and predicted values when a rich classes of approximating distributions are considered. In this paper we propose Updating VB (UVB), a recursive algorithm used to update a sequence of VB posterior approximations in an online setting, with the computation of each posterior update requiring only the data observed since the previous update. An extension to the proposed algorithm, named UVB-IS, allows the user to trade accuracy for a substantial increase in computational speed through the use of importance sampling. The two methods and their properties are detailed in two separate simulation studies. Two empirical illustrations of the proposed UVB methods are provided, including one where a Dirichlet Process Mixture model with a novel posterior dependence structure is repeatedly updated in the context of predicting the future behaviour of vehicles on a stretch of the US Highway 101.},
   author = {Nathaniel Tomasetti and Catherine S. Forbes and Anastasios Panagiotelis},
   month = {8},
   title = {Updating Variational Bayes: Fast sequential posterior inference},
   url = {http://arxiv.org/abs/1908.00225},
   year = {2019},
}
@working_paper{Tran2021,
   abstract = {This tutorial gives a quick introduction to Variational Bayes (VB), also called Variational Inference or Variational Approximation, from a practical point of view. The paper covers a range of commonly used VB methods and an attempt is made to keep the materials accessible to the wide community of data analysis practitioners. The aim is that the reader can quickly derive and implement their first VB algorithm for Bayesian inference with their data analysis problem. An end-user software package in Matlab together with the documentation can be found at https://vbayeslab.github.io/VBLabDocs/},
   author = {Minh-Ngoc Tran and Trong-Nghia Nguyen and Viet-Hung Dao},
   month = {3},
   title = {A practical tutorial on Variational Bayes},
   url = {http://arxiv.org/abs/2103.01327},
   year = {2021},
}
@article{Wand2011,
   abstract = {We develop strategies for mean field variational Bayes approximate inference for Bayesian hierarchical models containing elaborate distributions. We loosely define elaborate distributions to be those having more complicated forms compared with common distributions such as those in the Normal and Gamma families. Examples are Asymmetric Laplace, Skew Normal and Generalized Extreme Value distributions. Such models suffer from the difficulty that the parameter updates do not admit closed form solutions. We circumvent this problem through a combination of (a) specially tailored auxiliary variables, (b) univariate quadrature schemes and (c) finite mixture approximations of troublesome density functions. An accuracy assessment is conducted and the new methodology is illustrated in an application. © 2011 International Society for Bayesian Analysis.},
   author = {Matthew P. Wand and John T. Ormerod and Simone A. Padoan and Rudolf Frühwirth},
   doi = {10.1214/11-BA631},
   issn = {1936-0975},
   issue = {4},
   journal = {Bayesian Analysis},
   keywords = {Auxiliary mixture sampling,Bayesian inference,Quadrature,Variational methods},
   month = {12},
   pages = {847-900},
   title = {Mean Field Variational Bayes for Elaborate Distributions},
   volume = {6},
   url = {https://projecteuclid.org/journals/bayesian-analysis/volume-6/issue-4/Mean-Field-Variational-Bayes-for-Elaborate-Distributions/10.1214/11-BA631.full},
   year = {2011},
}
@article{Yang2020,
   abstract = {In high-dimensions, the prior tails can have a significant effect on both posterior computation and asymptotic concentration rates. To achieve optimal rates while keeping the posterior computations relatively simple, an empirical Bayes approach has recently been proposed, featuring thin-tailed conjugate priors with data-driven centers. While conjugate priors ease some of the computational burden, Markov chain Monte Carlo methods are still needed, which can be expensive when dimension is high. In this paper, we develop a variational approximation to the empirical Bayes posterior that is fast to compute and retains the optimal concentration rate properties of the original. In simulations, our method is shown to have superior performance compared to existing variational approximations in the literature across a wide range of high-dimensional settings.},
   author = {Yue Yang and Ryan Martin},
   month = {7},
   title = {Variational approximations of empirical Bayes posteriors in high-dimensional linear models},
   url = {http://arxiv.org/abs/2007.15930},
   year = {2020},
}
@working_paper{,
   abstract = {In this paper, we focus on variational Bayesian learning deterministic optimization methods for inference in biparametric exponential models where the parameters follow semiparametric regression structures. This combination of data models and algorithms contributes to solving real-world problems and reduces the computation time. This allows both the rapid exploration of many data models and the accurate estimation of the mean and variance functions through the connection between generalized linear models and graph theory. A simulation study was carried out to assess the performance of the deter-ministic approximation. Finally, herein, we present an application using macroeconomic data to emphasize the benefits of the proposed approach.},
   author = {Zarate-Solanom Hector and Edilberto Cepeda-Cuervo},
   doi = {10.20944/preprints202102.0551.v1},
   keywords = {Variational learning Bayes,biparametric exponential models,calculus of variations,optimization,semiparametric heterocedastic models},
   title = {Variational Bayesian Learning and Semiparametric Models on the Double Exponential Family},
   url = {www.preprints.org},
   year = {2021},
}
